<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Data Structures&amp;Algorithm Analysis in C++(Second Edition)</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="Data Structures&amp;Algorithm Analysis in C++(Second Edition)"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2012-09-16 00:56:36 HKT"/>
<meta name="author" content="Shi Shougang"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../assets/stylesheet.css" />
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>

</head>
<body>
<div id="org-div-home-and-up" style="text-align:right;font-size:70%;white-space:nowrap;">
 <a accesskey="h" href="../../index.html"> UP </a>
 |
 <a accesskey="H" href="../../index.html"> HOME </a>
</div>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Data Structures&amp;Algorithm Analysis in C++(Second Edition)</h1>



<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Sites</a>
<ul>
<li><a href="#sec-1-1">FTP</a></li>
<li><a href="#sec-1-2">author homepage</a></li>
<li><a href="#sec-1-3">source codes</a></li>
</ul>
</li>
<li><a href="#sec-2">References</a>
<ul>
<li><a href="#sec-2-1">Chapter 1</a></li>
<li><a href="#sec-2-2">Chapter 2</a></li>
<li><a href="#sec-2-3">Chapter 4: Trees</a></li>
<li><a href="#sec-2-4">CHAPTER 5: HASHING</a></li>
<li><a href="#sec-2-5">CHAPTER 6: PRIORITY QUEUES (HEAPS)</a></li>
<li><a href="#sec-2-6">CHAPTER 7: SORTING</a></li>
<li><a href="#sec-2-7">CHAPTER 8: THE DISJOINT SET ADT</a></li>
<li><a href="#sec-2-8">CHAPTER 9: GRAPH ALGORITHMS</a></li>
<li><a href="#sec-2-9">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</a></li>
</ul>
</li>
<li><a href="#sec-3">Notes</a>
<ul>
<li><a href="#sec-3-1">Chapter 1: INTRODUCTION</a></li>
<li><a href="#sec-3-2">CHAPTER 2: ALGORITHM ANALYSIS</a></li>
<li><a href="#sec-3-3">Chapter 3: Lists, Stacks, and Queues</a></li>
<li><a href="#sec-3-4">Chapter 4: Trees</a></li>
<li><a href="#sec-3-5">CHAPTER 5: HASHING</a></li>
<li><a href="#sec-3-6">CHAPTER 6: PRIORITY QUEUES (HEAPS)</a></li>
<li><a href="#sec-3-7">CHAPTER 7: SORTING</a></li>
<li><a href="#sec-3-8">CHAPTER 8: THE DISJOINT SET ADT</a></li>
<li><a href="#sec-3-9">CHAPTER 9: GRAPH ALGORITHMS</a></li>
<li><a href="#sec-3-10">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">Sites</h2>
<div class="outline-text-2" id="text-1">


</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1">FTP</h3>
<div class="outline-text-3" id="text-1-1">

<p>ftp.awl.com
</p></div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2">author homepage</h3>
<div class="outline-text-3" id="text-1-2">

<p><a href="http://users.cis.fiu.edu/~weiss/">http://users.cis.fiu.edu/~weiss/</a>
</p></div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3">source codes</h3>
<div class="outline-text-3" id="text-1-3">

<ul>
<li><a href="http://users.cis.fiu.edu/~weiss/dsaa_c++/code/">http://users.cis.fiu.edu/~weiss/dsaa_c++/code/</a>
</li>
<li><a href="http://users.cis.fiu.edu/~weiss/dsaa_c++/code/g++.txt">http://users.cis.fiu.edu/~weiss/dsaa_c++/code/g++.txt</a>
</li>
<li><a href="http://users.cis.fiu.edu/~weiss/adspc++2/code/">http://users.cis.fiu.edu/~weiss/adspc++2/code/</a>
</li>
</ul>

<ul>
<li id="sec-1-3-1">codes review<br/>
Global Utilities

<p>
StartConv.h 
EndConv.h 
getline.cpp
Chapter 1: Arrays, Pointers, Structures
</p>
<p>
ArrayDemo.cpp 
GetInts.cpp 
TestString.cpp 
TestSwap.cpp
Chapter 2: Classes
</p>
<p>
IntCell.h  IntCell.cpp  TestIntCell.cpp 
BuggyIntCell.cpp 
DeepIntCell.cpp 
Rational.h  Rational.cpp  RatMain.cpp 
mystring.h  string.cpp
Chapter 3: Templates
</p>
<p>
FuncTemplates.cpp 
InsSort.cpp 
MemoryCell.h  MemoryCell.cpp  TestMemoryCell.cpp 
Matrix.h 
vector.h  vector.cpp
Chapter 4: Inheritance
</p>
<p>
Except.h 
Shape.cpp  
Hiding.cpp 
StaticBinding.cpp
Chapter 5: Patterns
</p>
<p>
Rectangle.cpp 
Wrapper.h 
Ambiguity.cpp 
AutoPtr.cpp 
StorageCell.h TestStorageCell.cpp 
Iterator1.cpp 
Iterator2.cpp 
Iterator3.cpp 
pair.h 
Observer.cpp
Chapter 6: Running Times
</p>
<p>
MaxSum.cpp 
BinarySearch.cpp
Chapter 7: STL
</p>
<p>
vector.h  vector.cpp 
functional.h TestFunctional.cpp 
algorithm.h 
SimpleSetDemo.cpp 
TestPQ.cpp
Chapter 8: Recursion
</p>
<p>
RecSum.cpp 
PrintInt.cpp 
BinarySearchRec.cpp 
Ruler.java 
FractalStar.java 
Math.cpp 
MaxSum.cpp 
MkChnge.cpp 
TicTacSlow.cpp
Chapter 9: Sorting
</p>
<p>
Duplicate.cpp 
Sort.h  TestSort.cpp
Chapter 10: Randomization
</p>
<p>
Random.h  Random.cpp  RandTest.cpp 
Permute.cpp 
Math.cpp
Chapter 11: Fun and Games
</p>
<p>
WordSrch.cpp 
TicTac.cpp
Chapter 12: Applications of Stacks &ndash; Compilers and Parsing
</p>
<p>
Tokenizer.h  Tokenizer.cpp  Balance.cpp 
Infix.cpp
Chapter 13: Utilities
</p>
<p>
Hzip.cpp 
Tokenizer.h  Tokenizer.cpp  Xref.cpp
Chapter 14: Simulation
</p>
<p>
Josephus.cpp 
Modems.cpp
Chapter 15: Shortest Path Algorithms
</p>
<p>
Paths.cpp Note for DotNet users: on line 40 of PairingHeap.cpp, insert the word typename in front of the line so it appears as typename PairingHeap::Position
Chapter 16: Stacks
</p>
<p>
StackAr.h  StackAr.cpp  TestStackAr.cpp 
StackLi.h  StackLi.cpp  TestStackLi.cpp 
QueueAr.h  QueueAr.cpp  TestQueueAr.cpp 
QueueLi.h  QueueLi.cpp  TestQueueLi.cpp
Chapter 17: Linked Lists
</p>
<p>
LinkedList.h  LinkedList.cpp  SortLinkedList.h  SortLinkedList.cpp  TestLinkedList.cpp 
list.h  list.cpp  TestList.cpp
Chapter 18: Trees
</p>
<p>
BinaryTree.h  BinaryTree.cpp 
Iterate.h  Iterate.cpp  TestBinaryTree.cpp
Chapter 19: Search Trees
</p>
<p>
BinarySearchTree.h  BinarySearchTree.cpp  TestBinarySearchTree.cpp 
RedBlackTree.h  RedBlackTree.cpp  TestRedBlackTree.cpp 
AATree.h  AATree.cpp  TestAATree.cpp 
set.h  set.cpp  TestSet.cpp 
map.h  map.cpp  TestMap.cpp
Chapter 20: Hash Tables
</p>
<p>
QuadraticProbing.h  QuadraticProbing.cpp  TestQuadraticProbing.cpp
Chapter 21: Heaps
</p>
<p>
BinaryHeap.h  BinaryHeap.cpp  TestBinaryHeap.cpp 
queue.h  queue.cpp  TestQueue.cpp
Chapter 22: Splay Trees
</p>
<p>
SplayTree.h  SplayTree.cpp  TestSplayTree.cpp
Chapter 23: Pairing Heaps
</p>
<p>
PairingHeap.h  PairingHeap.cpp  TestPairingHeap.cpp Note for DotNet users: on line 40 of PairingHeap.cpp, insert the word typename in front of the line so it appears as typename PairingHeap::Position
Chapter 24: Disjoint Sets
</p>
<p>
DisjSets.h  DisjSets.cpp  TestDisjSets.cpp
Appendix D: Primitive Arrays
</p>
<p>
PointerHopping.cpp
</p>
</li>
</ul>
</div>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2">References</h2>
<div class="outline-text-2" id="text-2">


</div>

<div id="outline-container-2-1" class="outline-3">
<h3 id="sec-2-1">Chapter 1</h3>
<div class="outline-text-3" id="text-2-1">

<p>There are many good textbooks covering the mathematics reviewed in this chapter. A small subset
is <code>[1], [2], [3], [11], [13], and [14].</code> Reference <code>[11]</code> is specifically geared toward the analysis
of algorithms. It is the first volume of a three-volume series that will be cited throughout this
text. More advanced material is covered in <code>[6]</code>.
Throughout this book we will assume a knowledge of C <code>[10]</code>. Occasionally, we add a feature where
necessary for clarity. We also assume familiarity with pointers and recursion (the recursion
summary in this chapter is meant to be a quick review). We will attempt to provide hints on their
use where appropriate throughout the textbook. Readers not familiar with these should consult
<code>[4], [8], [12]</code>, or any good intermediate programming textbook.
</p>
<p>
General programming style is discussed in several books. Some of the classics are <code>[5], [7]</code>, and
<code>[9]</code>.
</p>
<ol>
<li>M. O. Albertson and J. P. Hutchinson, Discrete Mathematics with Algorithms, John Wiley &amp; Sons,
   New York, 1988.
</li>
<li>Z. Bavel, Math Companion for Computer Science, Reston Publishing Company, Reston, Va., 1982.
</li>
<li>R. A. Brualdi, Introductory Combinatorics, North-Holland, New York, 1977.
</li>
<li>W. H. Burge, Recursive Programming Techniques, Addison-Wesley, Reading, Mass., 1975.
</li>
<li>E. W. Dijkstra, A Discipline of Programming, Prentice Hall, Englewood Cliffs, N.J., 1976.
</li>
<li>R. L. Graham, D. E. Knuth, and O. Patashnik, Concrete Mathematics, Addison-Wesley, Reading,
   Mass., 1989.
</li>
<li>D. Gries, The Science of Programming, Springer-Verlag, New York, 1981.
</li>
<li>P. Helman and R. Veroff, Walls and Mirrors: Intermediate Problem Solving and Data Structures,
   2d ed., Benjamin Cummings Publishing, Menlo Park, Calif., 1988.
</li>
<li>B. W. Kernighan and P. J. Plauger, The Elements of Programming Style, 2d ed., McGraw- Hill,
   New York, 1978.
</li>
<li>B. W. Kernighan and D. M. Ritchie, The C Programming Language, 2d ed., Prentice Hall,
    Englewood Cliffs, N.J., 1988.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol. 1: Fundamental Algorithms, 2d ed.,
    Addison-Wesley, Reading, Mass., 1973.
</li>
<li>E. Roberts, Thinking Recursively, John Wiley &amp; Sons, New York, 1986.
</li>
<li>F. S. Roberts, Applied Combinatorics, Prentice Hall, Englewood Cliffs, N.J., 1984.
</li>
<li>A. Tucker, Applied Combinatorics, 2d ed., John Wiley &amp; Sons, New York, 1984.
</li>
</ol>

</div>

</div>

<div id="outline-container-2-2" class="outline-3">
<h3 id="sec-2-2">Chapter 2</h3>
<div class="outline-text-3" id="text-2-2">

<ol>
<li>A. V. Aho, J. E. Hopcroft, and J. D. Ullman, The Design and
   Analysis of Computer Algorithms, Addison-Wesley, Reading, Mass., 1974.
</li>
<li>J. L. Bentley, Writing Efficient Programs, Prentice Hall, Englewood Cliffs, N.J., 1982.
</li>
<li>J. L. Bentley, Programming Pearls, Addison-Wesley, Reading, Mass., 1986.
</li>
<li>J. L. Bentley, More Programming Pearls, Addison-Wesley, Reading, Mass., 1988.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 1: Fundamental Algorithms, 2d ed., Addison-Wesley, Reading, Mass., 1973.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 2: Seminumerical Algorithms, 2d ed., Addison-Wesley, Reading, Mass., 1981.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 3: Sorting and Searching, Addison-Wesley, Reading, Mass., 1975.
</li>
<li>D. E. Knuth, "Big Omicron and Big Omega and Big Theta," ACM SIGACT News, 8 (1976), 18-23.
</li>
</ol>


</div>

</div>

<div id="outline-container-2-3" class="outline-3">
<h3 id="sec-2-3">Chapter 4: Trees</h3>
<div class="outline-text-3" id="text-2-3">


More information on binary search trees, and in particular the mathematical
properties of trees can be found in the two books by Knuth [23] and [24].
Several papers deal with the lack of balance caused by biased deletion algorithms
in binary search trees. Hibbard's paper [20] proposed the original deletion
algorithm and established that one deletion preserves the randomness of the
trees. A complete analysis has been performed only for trees with three [21] and
four nodes[5]. Eppinger's paper [15] provided early empirical evidence of
nonrandomness, and the papers by Culberson and Munro, [11], [12], provide some
analytical evidence (but not a complete proof for the general case of intermixed
insertions and deletions).<br>

AVL trees were proposed by Adelson-Velskii and Landis [1]. Simulation results for
AVL trees, and variants in which the height imbalance is allowed to be at most
k for various values of k, are presented in [22]. A deletion algorithm for AVL
trees can be found in [24]. Analysis of the averaged depth of AVL trees is
incomplete, but some results are contained in [25].
[3] and [9] considered self-adjusting trees like the type in Section 4.5.1. Splay
trees are described in [29].<br>

B-trees first appeared in [6]. The implementation described in the original paper
allows data to be stored in internal nodes as well as leaves. The data structure
we have described is sometimes known as a B+ tree. A survey of the different
types of B-trees is presented in [10]. Empirical results of the various schemes
is reported in [18]. Analysis of 2-3 trees and B-trees can be found in [4], [14],
and [33].<br>

Exercise 4.14 is deceptively difficult. A solution can be found in [16]. Exercise
4.26 is from [32]. Information on B*-trees, described in Exercise 4.38, can be
found in [13]. Exercise 4.42 is from [2]. A solution to Exercise 4.43 using 2n -6
rotations is given in [30]. Using threads, a la Exercise 4.45, was first proposed
in [28]. k-d trees were first proposed in [7]. Their major drawback is that both
deletion and balancing are difficult. [8] discusses k-d trees and other methods
used for multidimensional searching.<br>

Other popular balanced search trees are red-black trees [19] and weight-balanced
trees [27]. More balanced tree schemes can be found in the books [17], [26], and
[31].<br>
<ol>
<li>G. M. Adelson-Velskii and E. M. Landis, "An Algorithm for the
   Organization of Information," Soviet Math. Doklady 3 (1962), 1259-1263.
</li>
<li>A. V. Aho, J. E. Hopcroft, and J. D. Ullman, The Design and
   Analysis of Computer Algorithms, Addison-Wesley, Reading, MA, 1974.
</li>
<li>B. Allen and J. I. Munro, "Self Organizing Search Trees," Journal
   of the ACM, 25 (1978), 526-535.
</li>
<li>R. A. Baeza-Yates, "Expected Behaviour of B+- trees under Random
   Insertions," Acta Informatica 26 (1989), 439-471.
</li>
<li>R. A. Baeza-Yates, "A Trivial Algorithm Whose Analysis Isn't: A
   Continuation," BIT 29 (1989), 88-113.
</li>
<li>R. Bayer and E. M. McGreight, "Organization and Maintenance of
   Large Ordered Indices," Acta Informatica 1 (1972), 173-189.
</li>
<li>J. L. Bentley, "Multidimensional Binary Search Trees Used for
   Associative Searching," Communications of the ACM 18 (1975), 509-517.
</li>
<li>J. L. Bentley and J. H. Friedman, "Data Structures for Range
   Searching," Computing Surveys 11 (1979), 397-409.
</li>
<li>J. R. Bitner, "Heuristics that Dynamically Organize Data
   Structures," SIAM Journal on Computing 8 (1979), 82-110.
</li>
<li>D. Comer, "The Ubiquitous B-tree," Computing Surveys 11 (1979), 121-137.
</li>
<li>J. Culberson and J. I. Munro, "Explaining the Behavior of Binary
    Search Trees under Prolonged Updates: A Model and Simulations,"
    Computer Journal 32 (1989), 68-75.
</li>
<li>J. Culberson and J. I. Munro, "Analysis of the Standard Deletion
    Algorithms' in Exact Fit Domain Binary Search Trees," Algorithmica 5 (1990) 295-311.
</li>
<li>K. Culik, T. Ottman, and D. Wood, "Dense Multiway Trees," ACM
    Transactions on Database Systems 6 (1981), 486-512.
</li>
<li>B. Eisenbath, N. Ziviana, G. H. Gonnet, K. Melhorn, and D. Wood,
    "The Theory of Fringe Analysis and its Application to 2-3 Trees and B-trees," Information and Control 55 (1982), 125-174.
</li>
<li>J. L. Eppinger, "An Empirical Study of Insertion and Deletion in
    Binary Search Trees," Communications of the ACM 26 (1983), 663-669.
</li>
<li>P. Flajolet and A. Odlyzko, "The Average Height of Binary Trees
    and Other Simple Trees," Journal of Computer and System Sciences 25 (1982), 171-213.
</li>
<li>G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data
    Structures, second edition, Addison-Wesley, Reading, MA, 1991.
</li>
<li>E. Gudes and S. Tsur, "Experiments with B-tree Reorganization,"
    Proceedings of ACM SIGMOD Symposium on Management of Data (1980), 200-206.
</li>
<li>L. J. Guibas and R. Sedgewick, "A Dichromatic Framework for
    Balanced Trees," Proceedings of the Nineteenth Annual IEEE
    Symposium on Foundations of Computer Science (1978), 8-21.
</li>
<li>T. H. Hibbard, "Some Combinatorial Properties of Certain Trees
    with Applications to Searching and Sorting," Journal of the ACM 9 (1962), 13-28.
</li>
<li>A. T. Jonassen and D. E. Knuth, "A Trivial Algorithm Whose
    Analysis Isn't," Journal of Computer and System Sciences 16 (1978), 301-322.
</li>
<li>P. L. Karlton, S. H. Fuller, R. E. Scroggs, and E. B. Kaehler,
    "Performance of Height Balanced Trees," Communications of the ACM 19 (1976), 23-28.
</li>
<li>D. E. Knuth, The Art of Computer Programming: Volume 1:
    Fundamental Algorithms, second edition, Addison-Wesley, Reading, MA, 1973.
</li>
<li>D. E. Knuth, The Art of Computer Programming: Volume 3: Sorting
    and Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>K. Melhorn, "A Partial Analysis of Height-Balanced Trees under
    Random Insertions and Deletions," SIAM Journal of Computing 11 (1982), 748-760.
</li>
<li>K. Melhorn, Data Structures and Algorithms 1: Sorting and
    Searching, Springer-Verlag, Berlin, 1984.
</li>
<li>J. Nievergelt and E. M. Reingold, "Binary Search Trees of Bounded
    Balance," SIAM Journal on Computing 2 (1973), 33-43.
</li>
<li>A. J. Perlis and C. Thornton, "Symbol Manipulation in Threaded
    Lists," Communications of the ACM 3 (1960), 195-204.
</li>
<li>D. D. Sleator and R. E. Tarjan, "Self-adjusting Binary Search
    Trees," Journal of ACM 32 (1985), 652-686.
</li>
<li>D. D. Sleator, R. E. Tarjan, and W. P. Thurston, "Rotation
    Distance, Triangulations, and Hyperbolic Geometry," Journal of AMS
    (1988), 647-682.

</li>
<li>H. F. Smith, Data Structures-Form and Function, Harcourt Brace
    Jovanovich, 1987.
</li>
<li>R. E. Tarjan, "Sequential Access in Splay Trees Takes Linear
    Time," Combinatorica 5 (1985), 367-378.
</li>
<li>A. C. Yao, "On Random 2-3 trees," Acta Informatica 9 (1978), 159-170.
</li>
</ol>



</div>

</div>

<div id="outline-container-2-4" class="outline-3">
<h3 id="sec-2-4">CHAPTER 5: HASHING</h3>
<div class="outline-text-3" id="text-2-4">


Despite the apparent simplicity of hashing, much of the analysis is quite difficult and there are
still many unresolved questions. There are also many interesting theoretical issues, which
generally attempt to make it unlikely that the worst-case
possibilities of hashing arise.<br>

An early paper on hashing is [17]. A wealth of information on the subject, including an analysis
of closed hashing with linear probing can be found in [11]. An excellent survey on the subject is
[14]; [15] contains suggestions, and pitfalls, for choosing hash functions. Precise analytic and
simulation results for all of the methods described in this chapter
can be found in [8].<br>

An analysis of double hashing can be found in [9] and [13]. Yet another collision resolution
scheme is coalesced hashing, as described in [18]. Yao [20] has shown that uniform hashing, in
which no clustering exists, is optimal with respect to cost of a successful search.<br>

If the input keys are known in advance, then perfect hash functions, which do not allow
collisions, exist [2], [7]. Some more complicated hashing schemes, for which the worst case
depends not on the particular input but on random numbers chosen by the algorithm, appear in [3]
and [4].<br>

Extendible hashing appears in [5], with analysis in [6] and [19].
One method of implementing Exercise 5.5 is described in [16]. Exercise 5.11 (a-d) is from [10].
Part (e) is from [12], and part (f) is from [1].
<ol>
<li>R. S. Boyer and J. S. Moore, "A Fast String Searching Algorithm,"
   Communications of the ACM 20 (1977), 762-772.
</li>
<li>J. L. Carter and M. N. Wegman, "Universal Classes of Hash
   Functions," Journal of Computer and System Sciences 18 (1979), 143-154.
</li>
<li>M. Dietzfelbinger, A. R. Karlin, K. Melhorn, F. Meyer auf der
   Heide, H. Rohnert, and R. E. Tarjan, Dynamic Perfect Hashing: Upper
   and Lower Bounds," Proceedings of the Twenty-ninth IEEE Symposium on Foundations of Computer Science (1988), 524-531.
</li>
<li>R. J. Enbody and H. C. Du, "Dynamic Hashing Schemes," Computing Surveys 20 (1988), 85-113.
</li>
<li>R. Fagin, J. Nievergelt, N. Pippenger, and H. R. Strong,
   "Extendible Hashing-A Fast Access Method for Dynamic Files," ACM Transactions on Database Systems 4 (1979), 315-344.
</li>
<li>P. Flajolet, "On the Performance Evaluation of Extendible Hashing
   and Trie Searching," Acta Informatica 20 (1983), 345-369.
</li>
<li>M. L. Fredman, J. Komlos, and E. Szemeredi, "Storing a Sparse Table
   with O(1) Worst Case Access Time," Journal of the ACM 31 (1984), 538-544.
</li>
<li>G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data
   Structures, second edition, Addison-Wesley, Reading, MA, 1991.
</li>
<li>L. J. Guibas and E. Szemeredi, "The Analysis of Double Hashing,"
   Journal of Computer and System Sciences 16 (1978), 226-274.
</li>
<li>R. M. Karp and M. O. Rabin, "Efficient Randomized Pattern-Matching
    Algorithms," Aiken Computer Laboratory Report TR-31-81, Harvard University, Cambridge, MA, 1981.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 3: Sorting and
    Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>D. E. Knuth, J. H. Morris, V. R. Pratt, "Fast Pattern Matching in
    Strings," SIAM Journal on Computing 6 (1977), 323-350.
</li>
<li>G. Lueker and M. Molodowitch, "More Analysis of Double Hashing,"
    Proceedings of the Twentieth ACM Symposium on Theory of Computing (1988), 354-359.
</li>
<li>W. D. Maurer and T. G. Lewis, "Hash Table Methods," Computing Surveys 7 (1975), 5-20.
</li>
<li>B. J. McKenzie, R. Harries, and T. Bell, "Selecting a Hashing
    Algorithm," Software&ndash;Practice and Experience 20 (1990), 209-224.
</li>
<li>R. Morris, "Scatter Storage Techniques," Communications of the ACM 11 (1968), 38-44.
</li>
<li>W. W. Peterson, "Addressing for Random Access Storage," IBM
    Journal of Research and Development 1 (1957), 130-146.
</li>
<li>J. S. Vitter, "Implementations for Coalesced Hashing,"
    Communications of the ACM 25 (1982), 911-926.
</li>
<li>A. C. Yao, "A Note on The Analysis of Extendible Hashing,"
    Information Processing Letters 11 (1980), 84-86.
</li>
<li>A. C. Yao, "Uniform Hashing is Optimal," Journal of the ACM 32
    (1985), 687-693.
</li>
</ol>


</div>

</div>

<div id="outline-container-2-5" class="outline-3">
<h3 id="sec-2-5">CHAPTER 6: PRIORITY QUEUES (HEAPS)</h3>
<div class="outline-text-3" id="text-2-5">


The binary heap was first described in [21]. The linear-time algorithm
for its construction is from [9].<br>

The first description of d -heaps was in [14]. Leftist heaps were invented by Crane [7] and
described in Knuth [15]. Skew heaps were developed by Sleator and Tarjan [17]. Binomial queues
were invented by Vuillemin [20]; Brown provided a detailed analysis and empirical study showing
that they perform well in practice [2], if carefully implemented.<br>

Exercise 6.7 (b-c) is taken from [12]. A method for constructing binary heaps that uses about
1.52n comparisons on average is described in [16]. Lazy deletion in leftist heaps (Exercise 6.21)
is from [6]. A solution to Exercise 6.33 can be found in [5].<br>

Min-max heaps (Exercise 6.15) were originally described in [1]. More efficient implementation of
the operations is given in [13] and [18]. An alternate representation for double ended priority
queues is the deap. Details can be found in [3] and [4].<br>

A theoretically interesting priority queue representation is the Fibonacci heap [11], which we
will describe in Chapter 11. The Fibonacci heap allows all operations to be performed in O(1)
amortized time, except for deletions, which are O(log n). Relaxed heaps [8] achieve identical
bounds in the worst case. Another interesting implementation is the pairing heap [10]. Finally, a priority queue that works when the data consists of small integers is described in [19].
<ol>
<li>M. D. Atkinson, J. R. Sack, N. Santoro, and T. Strothotte, "Min-Max
   Heaps and Generalized Priority Queues," Communications of the ACM 29 (1986), 996-1000.
</li>
<li>M. R. Brown, "Implementation and Analysis of Binomial Queue
   Algorithms," SIAM Journal on Computing 7 (1978), 298-319.
</li>
<li>S. Carlsson, "The Deap&ndash;A Double-ended Heap to Implement
   Double-ended Priority Queues," Information Processing Letters 26 (1987), 33-36.
</li>
<li>S. Carlsson, J. Chen, and T. Strothotte, "A Note on the
   Construction of the Data Structure 'Deap'," Information Processing Letters 31 (1989), 315-317.
</li>
<li>S. Carlsson, J. I. Munro, and P. V. Poblete, "An Implicit Binomial
   Queue with Constant Insertion Time," Proceedings of First Scandinavian Workshop on Algorithm Theory, 1988, 1-13.
</li>
<li>D. Cheriton and R. E. Tarjan, "Finding Minimum Spanning Trees,"
   SIAM Journal on Computing 5 (1976), 724-742.
</li>
<li>C. A. Crane, "Linear Lists and Priority Queues as Balanced Binary
   Trees," Technical Report STAN-CS-72-259, Computer Science Department, Stanford University, Stanford, CA, 1972.
</li>
<li>J. R. Driscoll, H. N. Gabow, R. Shrairman, and R. E. Tarjan,
   "Relaxed Heaps: An Alternative to Fibonacci Heaps with Applications
   to Parallel Computation," Communications of the ACM 31 (1988), 1343-1354.
</li>
<li>R. W. Floyd, "Algorithm 245: Treesort 3:", Communications of the ACM 7 (1964), 701.
</li>
<li>M. L. Fredman, R. Sedgewick, D. D. Sleator, and R. E. Tarjan, "The
    Pairing Heap: A New Form of Self-adjusting Heap," Algorithmica 1 (1986), 111-129.
</li>
<li>M. L. Fredman and R. E. Tarjan, "Fibonacci Heaps and Their Uses in
    Improved Network Optimization Algorithms," Journal of the ACM 34 (1987), 596-615.
</li>
<li>G. H. Gonnet and J. I. Munro, "Heaps on Heaps," SIAM Journal on Computing 15 (1986), 964-971.
</li>
<li>A. Hasham and J. R. Sack, "Bounds for Min-max Heaps," BIT 27 (1987), 315-323.
</li>
<li>D. B. Johnson, "Priority Queues with Update and Finding Minimum
    Spanning Trees," Information Processing Letters 4 (1975), 53-57.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 3: Sorting and
    Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>C. J. H. McDiarmid and B. A. Reed, "Building Heaps Fast," Journal
    of Algorithms 10 (1989), 352-365.
</li>
<li>D. D. Sleator and R. E. Tarjan, "Self-adjusting Heaps," SIAM
    Journal on Computing 15 (1986), 52-69.
</li>
<li>T. Strothotte, P. Eriksson, and S. Vallner, "A Note on
    Constructing Min-max Heaps," BIT 29 (1989), 251-256.
</li>
<li>P. van Emde Boas, R. Kaas, E. Zijlstra, "Design and Implementation
    of an Efficient Priority Queue," Mathematical Systems Theory 10 (1977), 99-127.
</li>
<li>J. Vuillemin, "A Data Structure for Manipulating Priority Queues,"
    Communications of the ACM 21 (1978), 309-314.
</li>
<li>J. W. J. Williams, "Algorithm 232: Heapsort," Communications of
    the ACM 7 (1964), 347-348.
</li>
</ol>


</div>

</div>

<div id="outline-container-2-6" class="outline-3">
<h3 id="sec-2-6">CHAPTER 7: SORTING</h3>
<div class="outline-text-3" id="text-2-6">


Knuth's book [10] is a comprehensive, though somewhat dated, reference for sorting. Gonnet and
Baeza-Yates [4] has some more recent results, as well as a huge
bibliography.<br>

The original paper detailing Shellsort is [21]. The paper by Hibbard [5] suggested the use of the
increments 2k - 1 and tightened the code by avoiding swaps. Theorem 7.4 is from [12]. Pratt's
lower bound, which uses a more complex method than that suggested in the text, can be found in
[14]. Improved increment sequences and upper bounds appear in [9], [20], and [23]; matching lower
bounds have been shown in [24]. A recent unpublished result by Poonen shows that no increment
sequence gives an O(n log n) worst-case running time. An identical result was obtained
independently and appears in [13]. The average-case running time for Shellsort is still
unresolved. Yao [26] has performed an extremely complex analysis for the three-increment case.
The result has yet to be extended to more increments. Experiments with various increment
sequences appear in [22].<br>

Heapsort was invented by Williams [25]; Floyd [1] provided the linear-time algorithm for heap
construction. The analysis of its average case has only recently been obtained [15].<br>

An exact average-case analysis of mergesort has been claimed in [3]; the paper detailing the
results is forthcoming. An algorithm to perform merging in linear time without extra space is
described in [8].<br>

Quicksort is from Hoare [6]. This paper analyzes the basic algorithm, describes most of the
improvements, and includes the selection algorithm. A detailed analysis and empirical study was
the subject of Sedgewick's dissertation [19]. Many of the important results appear in the three
papers [16], [17], and [18].<br>

Decision trees and sorting optimality are discussed in Ford and Johnson [2]. This paper also
provides an algorithm that almost meets the lower bound in terms of number of comparisons (but
not other operations). This algorithm was eventually shown to be slightly suboptimal by Manacher
[11].<br>

External sorting is covered in detail in [10]. Stable sorting, described in Exercise 7.24, has
been addressed by Horvath [7].<br>

<ol>
<li>R. W. Floyd, "Algorithm 245: Treesort 3," Communications of the ACM 7 (1964), 701.
</li>
<li>L. R. Ford and S. M. Johnson, "A Tournament Problem," American Mathematics Monthly 66 (1959),387-389.
</li>
<li>M. Golin and R. Sedgewick, "Exact Analysis of Mergesort," Fourth
   SIAM Conference on Discrete Mathematics, 1988.
</li>
<li>G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data
   Structures, second edition, Addison-Wesley, Reading, MA, 1991.
</li>
<li>T. H. Hibbard, "An Empirical Study of Minimal Storage Sorting,"
   Communications of the ACM 6
</li>
<li>C. A. R. Hoare, "Quicksort," Computer Journal 5 (1962), 10-15.
</li>
<li>E. C. Horvath, "Stable Sorting in Asymptotically Optimal Time and
   Extra Space," Journal of the ACM 25 (1978), 177-199.
</li>
<li>B. Huang and M. Langston, "Practical In-place Merging,"
   Communications of the ACM 31 (1988), 348-352.
</li>
<li>J. Incerpi and R. Sedgewick, "Improved Upper Bounds on Shellsort,"
   Journal of Computer and System Sciences 31 (1985), 210-224.
</li>
<li>D. E. Knuth, The Art of Computer Programming. Volume 3: Sorting
    and Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>G. K. Manacher, "The Ford-Johnson Sorting Algorithm Is Not
    Optimal," Journal of the ACM 26 (1979), 441-456.
</li>
<li>A. A. Papernov and G. V. Stasevich, "A Method of Information
    Sorting in Computer Memories," Problems of Information Transmission 1 (1965), 63-75.
</li>
<li>C. G. Plaxton and T. Suel, "Improved Lower Bounds for Shellsort,"
    Proceedings of the Thirtythird Annual IEEE Symposium on the Foundations of Computer Science, (1992).
</li>
<li>V. R. Pratt, Shellsort and Sorting Networks, Garland Publishing,
    New York, 1979. (Originally presented as the author's Ph.D. thesis, Stanford University, 1971.)
</li>
<li>R. Schaffer and R. Sedgewick, "The Analysis of Heapsort," Journal of Algorithms, to appear.
</li>
<li>R. Sedgewick, "Quicksort with Equal Keys," SIAM Journal on Computing 6 (1977), 240-267.
</li>
<li>R. Sedgewick, "The Analysis of Quicksort Programs," Acta Informatica 7 (1977), 327-355.
</li>
<li>R. Sedgewick, "Implementing Quicksort Programs," Communications of
    the ACM 21 (1978), 847- 857.
</li>
<li>R. Sedgewick, Quicksort, Garland Publishing, New York, 1978.
    (Originally presented as the author's Ph.D. thesis, Stanford University, 1975.)
</li>
<li>R. Sedgewick, "A New Upper Bound for Shellsort," Journal of Algorithms 2 (1986), 159-173.
</li>
<li>D. L. Shell, "A High-Speed Sorting Procedure," Communications of the ACM 2 (1959), 30-32.
</li>
<li>M. A. Weiss, "Empirical Results on the Running Time of Shellsort,"
    Computer Journal 34 (1991), 88-91.
</li>
<li>M. A. Weiss and R. Sedgewick, "More On Shellsort Increment
    Sequences," Information Processing Letters 34 (1990), 267-270.
</li>
<li>M. A. Weiss and R. Sedgewick, "Tight Lower Bounds for Shellsort,"
    Journal of Algorithms 11 (1990), 242-251.
</li>
<li>J. W. J. Williams, "Algorithm 232: Heapsort," Communications of the ACM 7 (1964), 347-348.
</li>
<li>A. C. Yao, "An Analysis of (h, k, 1) Shellsort," Journal of
    Algorithms 1 (1980), 14-50.
</li>
</ol>


</div>

</div>

<div id="outline-container-2-7" class="outline-3">
<h3 id="sec-2-7">CHAPTER 8: THE DISJOINT SET ADT</h3>
<div class="outline-text-3" id="text-2-7">


Various solutions to the union/find problem can be found in [5], [8], and [10]. Hopcroft and
Ullman showed the O(m log* n) bound of Section 8.6. Tarjan [14] obtained the bound O(m (m,n)).
A more precise (but asymptotically identical) bound for m < n appears in [2] and [17]. Various
other strategies for path compression and unions also achieve the same bound; see [17] for
details.<br>

A lower bound showing that under certain restrictions (m (m,n)) time is required to
process m union/find operations was given by Tarjan [15]. Identical bounds under less restrictive
conditions have been recently shown in [6] and [13].<br>

Applications of the union/find data structure appear in [1] and [9]. Certain special cases of the
union/find problem can be solved in O(m) time [7]. This reduces the running time of several
algorithms, such as [1], graph dominance, and reducibility (see references in Chapter 9) by a
factor of (m,n). Others, such as [9] and the graph connectivity problem in this chapter, are
unaffected. The paper lists 10 examples. Tarjan has used path compression to obtain efficient
algorithms for several graph problems [16].<br>

Average-case results for the union/find problem appear in [4], [11], and [19]. Results bounding
the running time of any single operation (as opposed to the entire sequence) appear in [3] and
[12].<br>

Exercise 8.8 is solved in [18].

<ol>
<li>A. V. Aho, J. E. Hopcroft, J. D. Ullman, "On Finding Lowest Common
   Ancestors in Trees," SIAM Journal on Computing 5 (1976), 115-132.
</li>
<li>L. Banachowski, "A Complement to Tarjan's Result about the Lower
   Bound on the Complexity of the Set Union Problem," Information Processing Letters 11 (1980), 59-65.
</li>
<li>N. Blum, "On the Single-operation Worst-case Time Complexity of the
   Disjoint Set Union Problem," SIAM Journal on Computing 15 (1986), 1021-1024.
</li>
<li>J. Doyle and R. L. Rivest, "Linear Expected Time of a Simple Union
   Find Algorithm," Information Processing Letters 5 (1976), 146-148.
</li>
<li>M. J. Fischer, "Efficiency of Equivalence Algorithms," Complexity
   of Computer Computation (eds. R. E. Miller and J. W. Thatcher), Plenum Press, 1972, 153-168.
</li>
<li>M. L. Fredman and M. E. Saks, "The Cell Probe Complexity of Dynamic
   Data Structures," Proceedings of the Twenty-first Annual Symposium on Theory of Computing (1989), 345-354.
</li>
<li>H. N. Gabow and R. E. Tarjan, "A Linear-time Algorithm for a
   Special Case of Disjoint Set Union,"Journal of Computer and System Sciences 30 (1985), 209-221.
</li>
<li>B. A. Galler and M. J. Fischer, "An Improved Equivalence Algorithm," Communications of the ACM
</li>
</ol>

<p>7 (1964), 301-303.
</p><ol>
<li>J. E. Hopcroft and R. M. Karp, "An Algorithm for Testing the
   Equivalence of Finite Automata," Technical Report TR-71-114, Department of Computer Science, Cornell University, Ithaca, NY, 1971.
</li>
<li>J. E. Hopcroft and J. D. Ullman, "Set Merging Algorithms," SIAM
    Journal on Computing 2 (1973), 294-303.
</li>
<li>D. E. Knuth and A. Schonhage, "The Expected Linearity of a Simple
    Equivalence Algorithm," Theoretical Computer Science 6 (1978), 281-315.
</li>
<li>J. A. LaPoutre, "New Techniques for the Union-Find Problem,"
    Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms (1990), 54-63.
</li>
<li>J. A. LaPoutre, "Lower Bounds for the Union-Find and the
    Split-Find Problem on Pointer Machines," Proceedings of the Twenty
    Second Annual ACM Symposium on Theory of Computing (1990), 34-44.
</li>
<li>R. E. Tarjan, "Efficiency of a Good but Not Linear Set Union
    Algorithm," Journal of the ACM 22 (1975), 215-225.
</li>
<li>R. E. Tarjan, "A Class of Algorithms Which Require Nonlinear Time
    to Maintain Disjoint Sets,"Journal of Computer and System Sciences 18 (1979), 110-127.
</li>
<li>R. E. Tarjan, "Applications of Path Compression on Balanced
    Trees," Journal of the ACM 26 (1979), 690-715.
</li>
<li>R. E. Tarjan and J. van Leeuwen, "Worst Case Analysis of Set Union
    Algorithms," Journal of the ACM 31 (1984), 245-281.
</li>
<li>J. Westbrook and R. E. Tarjan, "Amortized Analysis of Algorithms
    for Set Union with Backtracking," SIAM Journal on Computing 18 (1989), 1-11.
</li>
<li>A. C. Yao, "On the Average Behavior of Set Merging Algorithms,"
    Proceedings of Eighth Annual ACM Symposium on the Theory of Computation (1976), 192-195.
</li>
</ol>



</div>

</div>

<div id="outline-container-2-8" class="outline-3">
<h3 id="sec-2-8">CHAPTER 9: GRAPH ALGORITHMS</h3>
<div class="outline-text-3" id="text-2-8">


Good graph theory textbooks include
[7], [12], [21], and [34]. More advanced topics, including
the more careful attention to running times, are covered in [36], [38], and [45].<br>

Use of adjacency lists was advocated in [23]. The topological sort algorithm is from [28], as
described in [31]. Dijkstra's algorithm appeared in [8]. The improvements using d-heaps and
Fibonacci heaps are described in [27] and [14], respectively. The shortest-path algorithm with
negative edge weights is due to Bellman [3]; Tarjan [45] describes a more efficient way to
guarantee termination.<br>

Ford and Fulkerson's seminal work on network flow is [13]. The idea of augmenting along shortest
paths or on paths admitting the largest flow increase is from [11]. Other approaches to the
problem can be found in [9], [30], and [20]. An algorithm for the min-cost flow problem can be
found in [18].<br>

An early minimum spanning tree algorithm can be found in [4]. Prim's algorithm is from [39];
Kruskal's algorithm appears in [32]. Two O(|E| log log |V|) algorithms are [5] and [46]. The
theoretically best-known algorithms appear in [14] and [16]. An empirical study of these
algorithms suggests that Prim's algorithm, implemented with decrease_key, is best in practice on
most graphs [37].<br>

The algorithm for biconnectivity is from [41]. The first linear-time strong components algorithm
(Exercise 9.28) appears in the same paper. The algorithm presented in the text is due to Kosaraju
(unpublished) and Sharir [40]. Other applications of depth-first search appear in [24], [25],
[42], and [43] (as mentioned in Chapter 8, the results in [42] and [43] have been improved, but
the basic algorithm is unchanged).<br>

The classic reference work for the theory of NP-complete problems is [19]. Additional material
can be found in [1]. The NP-completeness of satisfiability is shown in [6]. The other seminal
paper is [29], which showed the NP-completeness of 21 problems. An excellent survey of complexity
theory is [44]. An approximation algorithm for the traveling salesman problem, which generally
gives nearly optimal results, can be found in [35].<br>

A solution to Exercise 9.8 can be found in [2]. Solutions to the bipartite matching problem in
Exercise 9.13 can be found in [22] and [33]. The problem can be generalized by adding weights to
the edges and removing the restriction that the graph is bipartite. Efficient solutions for the
unweighted matching problem for general graphs are quite complex. Details can be found in [10],
[15], and [17].<br>

Exercise 9.35 deals with planar graphs, which commonly arise in practice. Planar graphs are very
sparse, and many difficult problems are easier on planar graphs. An example is the graph
isomorphism problem, which is solvable in linear time for planar graphs [26]. No polynomial time
algorithm is known for general graphs.<br>

<ol>
<li>A. V. Aho, J. E. Hopcroft, and J. D. Ullman, The Design and Analysis of Computer Algorithms,
</li>
</ol>

<p>Addison-Wesley, Reading, MA, 1974.
</p><ol>
<li>R. K. Ahuja, K. Melhorn, J. B. Orlin, and R. E. Tarjan, "Faster Algorithms for the Shortest
</li>
</ol>

<p>Path Problem," Journal of the ACM 37 (1990), 213-223.
</p><ol>
<li>R. E. Bellman, "On a Routing Problem," Quarterly of Applied Mathematics 16 (1958), 87-90.
</li>
<li>O. Boruvka, "Ojistém problému minimálním (On a Minimal Problem)," Práca Moravské
</li>
</ol>

<p>3 (1926), 37-58.
</p><ol>
<li>D. Cheriton and R. E. Tarjan, "Finding Minimum Spanning Trees," SIAM Journal on Computing 5
</li>
</ol>

<p>(1976), 724-742.
</p><ol>
<li>S. Cook, "The Complexity of Theorem Proving Procedures," Proceedings of the Third Annual ACM
</li>
</ol>

<p>Symposium on Theory of Computing (1971), 151-158.
</p><ol>
<li>N. Deo, Graph Theory wtth Applications to Engineering and Computer Science, Prentice Hall,
</li>
</ol>

<p>Englewood Cliffs, NJ, 1974.
</p><ol>
<li>E. W. Dijkstra, "A Note on Two Problems in Connexion with Graphs," Numerische Mathematik 1
</li>
</ol>

<p>(1959), 269-271.
</p><ol>
<li>E. A. Dinic, "Algorithm for Solution of a Problem of Maximum Flow in Networks with Power
</li>
</ol>

<p>Estimation," Soviet Mathematics Doklady 11 (1970), 1277-1280.
</p><ol>
<li>J. Edmonds, "Paths, Trees, and Flowers," Canadian Journal of Mathematics 17 (1965) 449-467.
</li>
<li>J. Edmonds and R. M. Karp, "Theoretical Improvements in Algorithmic Efficiency for Network
</li>
</ol>

<p>Flow Problems," Journal of the ACM 19 (1972), 248-264.
</p><ol>
<li>S. Even, Graph Algorithms, Computer Science Press, Potomac, MD, 1979.
</li>
<li>L. R. Ford, Jr. and D. R. Fulkerson, Flows in Networks, Princeton University Press,
</li>
</ol>

<p>Princeton, NJ, 1962.
</p><ol>
<li>M. L. Fredman and R. E. Tarjan, "Fibonacci Heaps and Their Uses in Improved Network
</li>
</ol>

<p>Optimization Algorithms," Journal of the ACM 34 (1987), 596-615.
</p><ol>
<li>H. N. Gabow, "Data Structures for Weighted Matching and Nearest Common Ancestors with
</li>
</ol>

<p>Linking," Proceedings of First Annual ACM-SIAM Symposium on Discrete
Algorithms (1990), 434-443.
</p><ol>
<li>H. N. Gabow, Z. Galil, T. H. Spencer, and R. E. Tarjan, "Efficient Algorithms for Finding
</li>
</ol>

<p>Minimum Spanning Trees on Directed and Undirected Graphs," Combinatorica 6 (1986), 109-122.
</p><ol>
<li>Z. Galil, "Efficient Algorithms for Finding Maximum Matchings in Graphs," ACM Computing
</li>
</ol>

<p>Surveys 18 (1986), 23-38.
</p><ol>
<li>Z. Galil and E. Tardos,"An O(n2(m + n log n)log n) Min-Cost Flow Algorithm," Journal of the
</li>
</ol>

<p>ACM 35 (1988), 374-386.
</p><ol>
<li>M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to the Theory of NPCompleteness,
</li>
</ol>

<p>Freeman, San Francisco, 1979.
</p><ol>
<li>A. V. Goldberg and R. E. Tarjan, "A New Approach to the Maximum-Flow Problem," Journal of the
</li>
</ol>

<p>ACM 35 (1988), 921-940.
</p><ol>
<li>F. Harary, Graph Theory, Addison-Wesley, Reading, MA, 1969.
</li>
<li>J. E. Hopcroft and R. M. Karp, "An n5/2 Algorithm for Maximum Matchings in Bipartite Graphs,"
</li>
</ol>

<p>SIAM Journal on Computing 2 (1973), 225-231.
</p><ol>
<li>J. E. Hopcroft and R. E. Tarjan, "Algorithm 447: Efficient Algorithms for Graph
</li>
</ol>

<p>Manipulation," Communications of the ACM 16 (1973), 372-378.
</p><ol>
<li>J. E. Hopcroft and R. E. Tarjan, "Dividing a Graph into Triconnected Components," SIAM
</li>
</ol>

<p>Journal on Computing 2 (1973), 135-158.
</p><ol>
<li>J. E. Hopcroft and R. E. Tarjan, "Efficient Planarity Testing,"
    Journal of the ACM 21 (1974),
</li>
<li>J. E. Hopcroft and J. K. Wong, "Linear Time Algorithm for Isomorphism of Planar Graphs,"
</li>
</ol>

<p>Proceedings of the Sixth Annual ACM Symposium on Theory of Computing (1974), 172-184.
</p><ol>
<li>D. B. Johnson, "Efficient Algorithms for Shortest Paths in Sparse Networks," Journal of the
</li>
</ol>

<p>ACM 24 (1977), 1-13.
</p><ol>
<li>A. B. Kahn, "Topological Sorting of Large Networks," Communications of the ACM 5 (1962), 558-
</li>
<li>
</li>
<li>R. M. Karp, "Reducibility among Combinatorial Problems," Complexity of Computer Computations
</li>
</ol>

<p>(eds. R. E. Miller and J. W. Thatcher), Plenum Press, New York, 1972, 85-103.
</p><ol>
<li>A. V. Karzanov, "Determining the Maximal Flow in a Network by the Method of Preflows," Soviet
</li>
</ol>

<p>Mathematics Doklady 15 (1974), 434-437.
</p><ol>
<li>D. E. Knuth, The Art of Computer Programming, Vol. 1: Fundamental Algorithms, second edition,
</li>
</ol>

<p>Addison-Wesley, Reading, MA, 1973.
</p><ol>
<li>J. B. Kruskal, Jr. "On the Shortest Spanning Subtree of a Graph and the Traveling Salesman
</li>
</ol>

<p>Problem," Proceedings of the American Mathematical Society 7 (1956), 48-50.
</p><ol>
<li>H. W. Kuhn, "The Hungarian Method for the Assignment Problem," Naval Research Logistics
</li>
</ol>

<p>Quarterly 2 (1955), 83-97.
</p><ol>
<li>E. L. Lawler, Combinatorial Optimization: Networks and Matroids, Holt, Reinhart, and Winston,
</li>
</ol>

<p>New York, NY, 1976.
</p><ol>
<li>S. Lin and B. W. Kernighan, "An Effective Heuristic Algorithm for the Traveling Salesman
</li>
</ol>

<p>Problem," Operations Research 21 (1973), 498-516.
</p><ol>
<li>K. Melhorn, Data Structures and Algorithms 2: Graph Algorithms and NP-completeness, Springer-
</li>
</ol>

<p>Verlag, Berlin, 1984.
</p><ol>
<li>B. M. E. Moret and H. D. Shapiro, "An Empirical Analysis of Algorithms for Constructing a
</li>
</ol>

<p>Minimum Spanning Tree," Proceedings of the Second Workshop on Algorithms and Data Structures
(1991), 400-411.
</p><ol>
<li>C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization: Algorithms and Complexity,
</li>
</ol>

<p>Prentice Hall, Englewood Cliffs, NJ, 1982.
</p><ol>
<li>R. C. Prim, "Shortest Connection Networks and Some Generalizations," Bell System Technical
</li>
</ol>

<p>Journal 36 (1957), 1389-1401.
</p><ol>
<li>M. Sharir, "A Strong-Connectivity Algorithm and Its Application in Data Flow Analysis,"
</li>
</ol>

<p>Computers and Mathematics with Applications 7 (1981), 67-72.
</p><ol>
<li>R. E. Tarjan, "Depth First Search and Linear Graph Algorithms," SIAM Journal on Computing 1
</li>
</ol>

<p>(1972), 146-160.
</p><ol>
<li>R. E. Tarjan, "Testing Flow Graph Reducibility," Journal of Computer and System Sciences 9
</li>
</ol>

<p>(1974), 355-365.
</p><ol>
<li>R. E. Tarjan, "Finding Dominators in Directed Graphs," SIAM Journal on Computing 3 (1974),
</li>
</ol>

<p>62-89.
</p><ol>
<li>R. E. Tarjan, "Complexity of Combinatorial Algorithms," SIAM Review 20 (1978), 457-491.
</li>
<li>R. E. Tarjan, Data Structures and Network Algorithms, Society for Industrial and Applied
</li>
</ol>

<p>Mathematics, Philadelphia, PA, 1983.
</p><ol>
<li>A. C. Yao, "An O( |E | log log |V | ) Algorithm for Finding Minimum Spanning Trees,"
</li>
</ol>

<p>Information Processing Letters 4 (1975), 21-23.
</p>
</div>

</div>

<div id="outline-container-2-9" class="outline-3">
<h3 id="sec-2-9">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</h3>
<div class="outline-text-3" id="text-2-9">



</div>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3">Notes</h2>
<div class="outline-text-2" id="text-3">


</div>

<div id="outline-container-3-1" class="outline-3">
<h3 id="sec-3-1">Chapter 1: INTRODUCTION</h3>
<div class="outline-text-3" id="text-3-1">

<ul>
<li id="sec-3-1-1">1.2.3<br/>
<a href="http://orgmode.org/org.html#LaTeX-fragments">http://orgmode.org/org.html#LaTeX-fragments</a>


<p>
&sum;<sub>i=1</sub><sup>N</sup> i<sup>k</sup> &asymp; \frac{N<sup>k+1</sup>}{\abs{k+1}}  k &ne; -1
</p>
</li>
</ul>
<ul>
<li id="sec-3-1-2">1.3<br/>
When writhing recursive routines, it is crucial to keep in mind the
four basic rules of recursion:
<ol>
<li><b>Base cases</b>. You must always have some base cases, which can be
   solved without recursion.
</li>
<li><b>Making progress</b>. For the cases that are to be solved recursively,
   the recursive call must always be to a case that makes progress
   toward a base class.
</li>
<li><b>Design rule</b>. Assume that all the recursive calls work.
</li>
<li><b>Compound interest rule</b>. Never duplicate work by solving the same
   instance of a problem in separate recursive calls.
</li>
</ol>

</li>
</ul>
<ul>
<li id="sec-3-1-3">1.5.2<br/>
To summarize the parameter-passing options:
<ul>
<li>Call by value is appropriate for small objects that should not be
  altered by the function.
</li>
<li>Call by constant reference is appropriate for large objects that
  should not be altered by the function.
</li>
<li>Call by reference is appropriate for all objects that may be altered
  by the function.
</li>
</ul>


</li>
</ul>
<ul>
<li id="sec-3-1-4">1.5.5<br/>
This is a so-called <i>shallow copy</i>. Typically, we would expect a <i>deep copy</i>, in which a clone of the entire object is made.



</li>
</ul>
</div>

</div>

<div id="outline-container-3-2" class="outline-3">
<h3 id="sec-3-2">CHAPTER 2: ALGORITHM ANALYSIS</h3>
<div class="outline-text-3" id="text-3-2">

<ul>
<li id="sec-3-2-1">2.1<br/>
log<sup>k</sup> N = O(N) for any constant k. 

<p>
compute lim<sub>N&rarr;&infin;</sub> f(N)/g(N), using L<sup>'Hopital's</sup> rule if
necessary.
</p>
<p>
The rule is if lim<sub>N&rarr;&infin;</sub> f(N) = &infin; and
lim<sub>N&rarr;&infin;</sub> g(N) = &infin;, then lim<sub>N&rarr;&infin;</sub>
f(N)/g(N) = lim<sub>N&rarr;&infin;</sub>f<sup>'</sup>(N)/g<sup>'</sup>(N).
</p>
<p>
The limit can have four possible values:
</p><ul>
<li>The limit is 0: This means that f(N) = o(g(N)).
</li>
<li>The limit is c &ne; 0: This means that f(N) = &Theta;(g(N)).
</li>
<li>The limit is &infin;: This means that g(N) = o(f(N)).
</li>
<li>The limit is oscillates: There is no relation. 
</li>
</ul>


</li>
</ul>
<ul>
<li id="sec-3-2-2">2.4.2<br/>
<ul>
<li>RULE 1-FOR LOOPS:
</li>
</ul>

<p>The running time of a for loop is at most the running time of the statements
inside the for loop (including tests) times the number of iterations.
</p>
<ul>
<li>RULE 2-NESTED FOR LOOPS:
</li>
</ul>

<p>Analyze these inside out. The total running time of a statement inside a group of
nested for loops is the running time of the statement multiplied by the product
of the sizes of all the for loops.
</p>
<ul>
<li>RULE 3-CONSECUTIVE STATEMENTS:
</li>
</ul>

<p>These just add (which means that the maximum is the one that counts &ndash; see 1(a)
on page 16).
</p>
<ul>
<li>RULE 4-lF/ELSE:
</li>
</ul>

<p>For the fragment
</p>


<pre class="src src-c"><span style="color: #00ffff;">if</span>( cond )
S1
<span style="color: #00ffff;">else</span>
S2
</pre>

<p>
the running time of an if/else statement is never more than the running time of
the test plus the larger of the running times of S1 and S2.
</p></li>
</ul>
<ul>
<li id="sec-3-2-3">2.4.3<br/>
<ul>
<li>Algorithm 1 (O(N<sup>3</sup>))
</li>
</ul>




<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum1</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = 0; i &lt; a.size(); i++)
    <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">j</span> = i; j &lt; a.size(); j++){
      <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">thisSum</span> = 0;
      <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">k</span> = i; k &lt;= j; k++){
        thisSum += a[k];
      }
      <span style="color: #00ffff;">if</span>(maxSum &lt; thisSum)
        maxSum = thisSum;
    }
  <span style="color: #00ffff;">return</span> maxSum;
}
</pre>


<ul>
<li>Algorthm 2 (O(N<sup>2</sup>))
</li>
</ul>




<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum2</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = 0; i &lt; a.size(); i++){
    <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">thisSum</span> = 0;
    <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">j</span> = i; j &lt; a.size(); j++){
      thisSum += a[j];
      <span style="color: #00ffff;">if</span>(maxSum &lt; thisSum)
        maxSum = thisSum;
    }
  }
  <span style="color: #00ffff;">return</span> maxSum;
}
</pre>

<ul>
<li>Algorithm 3 (O(Nlog(N)))
</li>
</ul>




<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">max3</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">a</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">b</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">c</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">max</span>;
  <span style="color: #00ffff;">if</span>(a &gt; b)
    max = a;
  <span style="color: #00ffff;">else</span>
    max = b;
  <span style="color: #00ffff;">if</span>(max &lt; c)
    max = c;
  <span style="color: #00ffff;">return</span> max;
}

<span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubRec</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">left</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">right</span>)
{
  <span style="color: #00ffff;">if</span>(left == right)
    <span style="color: #00ffff;">if</span>(a[left] &gt; 0)
      <span style="color: #00ffff;">return</span> a[left];
    <span style="color: #00ffff;">else</span>
      <span style="color: #00ffff;">return</span> 0;

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">center</span> = (left + right) / 2;

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxLeftSum</span> = maxSubRec(a, left, center);
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxRightSum</span> = maxSubRec(a, center + 1, right);

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxLeftBorderSum</span> = 0, <span style="color: #eedd82;">leftBorderSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = center; i &gt;= left; i--){
    leftBorderSum += a[i];
    <span style="color: #00ffff;">if</span>(maxLeftBorderSum &lt; leftBorderSum)
      maxLeftBorderSum = leftBorderSum;
  }

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxRightBorderSum</span> = 0, <span style="color: #eedd82;">rightBorderSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = center+1; i &lt;= right; i++){
    rightBorderSum += a[i];
    <span style="color: #00ffff;">if</span>(maxRightBorderSum &lt; rightBorderSum)
      maxRightBorderSum = rightBorderSum;
  }

  <span style="color: #00ffff;">return</span> max3(maxLeftSum, maxRightSum, maxLeftBorderSum + maxRightBorderSum);
}


<span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum3</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #00ffff;">return</span> maxSubRec(a, 0, a.size() - 1);
}
</pre>


<ul>
<li>Algorithm 4 (O(log(N)))
</li>
</ul>




<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum4</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span> = 0;
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">thisSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = 0; i &lt; a.size(); i++){
    thisSum += a[i];
    <span style="color: #00ffff;">if</span>(thisSum &lt; 0)
      thisSum = 0;
    <span style="color: #00ffff;">if</span>(thisSum &gt; maxSum)
      maxSum = thisSum;

  }
  <span style="color: #00ffff;">return</span> maxSum;
}
</pre>


<ul>
<li>test<sub>main</sub>.cpp
</li>
</ul>




<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">main</span>(<span style="color: #98fb98;">int</span>  <span style="color: #eedd82;">argc</span>, <span style="color: #98fb98;">char</span> *<span style="color: #eedd82;">argv</span>[])
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">array</span>[] = { 4, -3, 5, -2, -1, 2, 6, -1};
  <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; <span style="color: #eedd82;">a</span>(array, array + <span style="color: #00ffff;">sizeof</span>(array) / <span style="color: #00ffff;">sizeof</span>(<span style="color: #98fb98;">int</span>));

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span>;
  maxSum = maxSubSum1(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum1 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  maxSum = maxSubSum2(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum2 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  maxSum = maxSubSum3(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum3 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  maxSum = maxSubSum4(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum4 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  <span style="color: #00ffff;">return</span> 0;
}
</pre>


</li>
</ul>
<ul>
<li id="sec-3-2-4">2.4.4<br/>
<ul>
<li>Binary Search(O(logN))
</li>
</ul>




<pre class="src src-c++"><span style="color: #00ffff;">template</span> &lt;<span style="color: #00ffff;">class</span> <span style="color: #98fb98;">Comparable</span>&gt;
<span style="color: #98fb98;">int</span> <span style="color: #87cefa;">binarySearch</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vecotr</span>&lt;<span style="color: #98fb98;">Comparable</span>&gt; &amp;<span style="color: #eedd82;">a</span>, <span style="color: #00ffff;">const</span> <span style="color: #98fb98;">Comparable</span> &amp;<span style="color: #eedd82;">x</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">left</span> = 0, <span style="color: #eedd82;">right</span> = a.size() - 1;

  <span style="color: #00ffff;">while</span>(left &lt; right){
    <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">mid</span> = (left + right) / 2;
    <span style="color: #00ffff;">if</span>(a[mid] == x)
      <span style="color: #00ffff;">return</span> mid;
    <span style="color: #00ffff;">else</span> <span style="color: #00ffff;">if</span>(a[mid] &gt; x)
      right = mid - 1;
    <span style="color: #00ffff;">else</span>
      left = mid + 1;
  }
  <span style="color: #00ffff;">return</span> NOT_FOUND; <span style="color: #ff7f24;">// </span><span style="color: #ff7f24;">NOT_FOUND s defined as -1</span>
}
</pre>

<ul>
<li>Euclid's Algorithm (O(logN))
</li>
</ul>




<pre class="src src-c++"><span style="color: #98fb98;">long</span> <span style="color: #87cefa;">gcd</span>(<span style="color: #98fb98;">long</span> <span style="color: #eedd82;">m</span>, <span style="color: #98fb98;">long</span> <span style="color: #eedd82;">n</span>) <span style="color: #ff7f24;">//</span><span style="color: #ff7f24;">assume m &gt; n</span>
{
  <span style="color: #98fb98;">long</span> <span style="color: #eedd82;">rem</span>;
  <span style="color: #00ffff;">while</span>(n =! 0){
    rem = m % n;
    m = n;
    n = rem;}
  <span style="color: #00ffff;">return</span> m;
}
</pre>


<ul>
<li>Exponentiation
</li>
</ul>




<pre class="src src-c++"><span style="color: #98fb98;">long</span> <span style="color: #87cefa;">pow</span>(<span style="color: #98fb98;">ling</span> <span style="color: #eedd82;">x</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">n</span>)
{
  <span style="color: #00ffff;">if</span>(n = 0)
    <span style="color: #00ffff;">return</span> 1;

  <span style="color: #00ffff;">if</span>(isEven(n))
    <span style="color: #00ffff;">return</span> pow(x * x, n / 2);
  <span style="color: #00ffff;">else</span>
    <span style="color: #00ffff;">return</span>  pow(x * x, n / 2) * x;
}  
</pre>


</li>
</ul>
</div>

</div>

<div id="outline-container-3-3" class="outline-3">
<h3 id="sec-3-3">Chapter 3: Lists, Stacks, and Queues</h3>
<div class="outline-text-3" id="text-3-3">





</div>

</div>

<div id="outline-container-3-4" class="outline-3">
<h3 id="sec-3-4">Chapter 4: Trees</h3>
<div class="outline-text-3" id="text-3-4">

</div>

</div>

<div id="outline-container-3-5" class="outline-3">
<h3 id="sec-3-5">CHAPTER 5: HASHING</h3>
<div class="outline-text-3" id="text-3-5">

</div>

</div>

<div id="outline-container-3-6" class="outline-3">
<h3 id="sec-3-6">CHAPTER 6: PRIORITY QUEUES (HEAPS)</h3>
<div class="outline-text-3" id="text-3-6">

</div>

</div>

<div id="outline-container-3-7" class="outline-3">
<h3 id="sec-3-7">CHAPTER 7: SORTING</h3>
<div class="outline-text-3" id="text-3-7">

</div>

</div>

<div id="outline-container-3-8" class="outline-3">
<h3 id="sec-3-8">CHAPTER 8: THE DISJOINT SET ADT</h3>
<div class="outline-text-3" id="text-3-8">

</div>

</div>

<div id="outline-container-3-9" class="outline-3">
<h3 id="sec-3-9">CHAPTER 9: GRAPH ALGORITHMS</h3>
<div class="outline-text-3" id="text-3-9">

</div>

</div>

<div id="outline-container-3-10" class="outline-3">
<h3 id="sec-3-10">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</h3>
<div class="outline-text-3" id="text-3-10">

</div>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2012-09-16 00:56:36 HKT</p>
<p class="author">Author: Shi Shougang</p>
<p class="creator">Org version 7.8.09 with Emacs version 24</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
