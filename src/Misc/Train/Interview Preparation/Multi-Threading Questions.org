#+SETUPFILE: ~/.emacs.d/src/org-templates/level-3.org
#+TITLE: Multi-Threading Questions
#+OPTIONS: num:nil H:2

* What is the difference between a thread and a process?
** heavyweight vs lightweight
Processes are always used to execute "heavyweight" jobs such as
running different applications, while threads are always used to
execute smaller "lightweight" jobs such as auto saving.
** create 
New threads are easily created, whereas new processes require
duplication of the parent process.

Creating a new process can be expensive. It takes time. (A call into
the operating system is needed, and if the process creation triggers
process rescheduling activity, the operating system's
context-switching mechanism will become involved.) IT takes memory.
(The entire process must be replicated.) 

Threads can be created without replicating an entire process.
** Sharing Resources
Independent processes share nothing. Each process runs in a separate
address space. Changes made to the parent processes donot affect the
child processes.

Threads share such process resources and global variables and file
descriptions.  If one thread
changes the value of any such resource, the change will be evident to
any other thread in the process, if anyone cares to look. The sharing
of process resources among threads is one of the multithreaded
programming model's major performance advantages, as well as one of
its most difficult programming aspects. Having all of this context
available to all threads in the same memory facilitates communication
between threads. However, at the same time, it makes it easy to
introduce errors of the sort in which one thread affects the value of
a variable used by another thread in ways the other thread did not
expect.

Then switching between threads is much simpler and faster then
switching between processes.
** Communication
Threads can directly communicate with other threads of its process;
processes must use inter-process communication mechanisms to
communicate with sibling processes.

Multiple processes can use any of the many other UNIX Interprocess
Communication (IPC) mechanisms: sockets, shared memory, and messages,
to name a few. The multiprocess version of our program uses shared
memory, but the other methods are equally valid. Even the waitpid call
in our program could be used to exchange information, if the program
checked its return value. However, in the multiprocess world, all
types of IPC involve a call into the operating system—to initialize
shared memory or a message structure, for instance. This makes
communication between processes more expensive than communication
between threads. Add to this the cost of
interprocess communication and synchronization of shared data, which
also may involve calls into the operating system kernel.

When processes
synchronize, they usually have to issue system calls, a relatively
expensive operation that involves trapping into the kernel. But
threads can synchronize by simply monitoring a variable—in other
words, staying within the user address space of the program.
** similarities
A thread can do anything that a process can do. They are both able to
reproduce(Processes create child process and threads create more
threads.), and they both have ID, priority and etc.

* primary benefits to multithreading
http://www.quora.com/What-is-the-difference-between-a-process-and-a-thread

There are four primary benefits to multithreading:

+ Programming abstraction. Dividing up work and assigning each
  division to a unit of execution (a thread) is a natural approach to
  many problems. Programming patterns that utilize this approach
  include the reactor, thread-per-connection, and thread pool
  patterns. Some, however, view threads as an anti-pattern. The
  inimitable Alan Cox summed this up well with the quote, "threads are
  for people who can't program state machines."

+ Parallelism. In machines with multiple processors, threads provide
  an efficient way to achieve true parallelism. As each thread
  receives its own virtualized processor and is an
  independently-schedulable entity, multiple threads may run on
  multiple processors at the same time, improving a system's
  throughput. To the extent that threads are used to achieve
  parallelism—that is, there are no more threads than processors—the
  "threads are for people who can't program state machines" quote does
  not apply.

+ Blocking I/O. Without threads, blocking I/O halts the whole process.
  This can be detrimental to both throughput and latency. In a
  multithreaded process, individual threads may block, waiting on I/O,
  while other threads make forward progress. Asynchronous &
  non-blocking I/O are alternative solutions to threads for this
  issue.

+ Memory savings. Threads provide an efficient way to share memory yet
  utilize multiple units of execution. In this manner they are an
  alternative to multiple processes.

The cost of these benefits of threading are increased complexity in
the form of needing to manage concurrency through mechanisms such as
mutexes and condition variables. Given the growing trend toward
processors sporting multiple cores and systems sporting multiple
processors, threading is only going to become a more important tool in
system programming.


Advantages and Disadvantages

Thread Advantages

+ Threads are memory efficient. Many threads can be efficiently contained within a single EXE, while each process can incur the overhead of an entire EXE.
+ Threads share a common program space, which among other things, means that messages can be passed by queuing only a pointer to the message. Since processes do not share a common program space, the kernel must either copy the entire message from process A's program space to process B's program space - a tremendous disadvantage for large messages, or provide some mechanism by which process B can access the message.
+ Thread task switching time is faster, since a thread has less context to save than a process.
+ With threads the kernel is linked in with the user code to create a single EXE. This means that all the kernel data structures like the ready queue are available for viewing with a debugger. This is not the case with a process, since the process is an autonomous application and the kernel is separate, which makes for a less flexible environment.

Thread Disadvantages

+ Threads are typically not loadable. That is, to add a new thread, you must add the new thread to the source code, then compile and link to create the new executable. Processes are loadable, thus allowing a multi-tasking system to be characterized dynamically. For example, depending upon system conditions, certain processes can be loaded and run to characterize the system. However, the same can be accomplished with threads by linking in all the possible threads required by the system, but only activating those that are needed, given the conditions. The really big advantage of loadability is that the process concept allows processes (applications) to be developed by different companies and offered as tools to be loaded and used by others in their multi-tasking applications.
+ Threads can walk over the data space of other threads. This cannot
  happen with processes. If an attempt is made to walk on another
  process an exception error will occur.

* What synchronization primitives do you know, tell difference between them.
* What is a deadlock and what is a livelock.
* What is a race condition.
* What does the term 'lock-free' mean.
* What is the best way to terminate a thread.
* Why you shouldn't use TerminateThread-esque functions.
* Write a thread-safe producer/consumer buffer that can be accessed by one or more producer/consumers
* When might you choose to use threads on a single CPU system?
* How would you measure the context switch overhead between threads?
* How would you make a MT-safe hash table, while allowing for maximal concurrency?
* What can go wrong when you write thread code and how can you guard against them?
Basically, the big three of threading problems are deadlock, races and
starvation. 

The simplest deadlock condition is when there are two threads and
thread A can't progress until thread B finishes, while thread B can't
progress until thread A finishes. This is usually because both need
the same two resources to progress, A has one and B has the other.
Various symmetry breaking algorithms can prevent this in the two
thread or larger circle cases. 

Races happen when one thread changes the state of some resource when
another thread is not expecting it (such as changing the contents of a
memory location when another thread is part way through reading, or
writing to that memory). Locking methods are the key here. (Some lock
free methods and containers are also good choices for this. As are
atomic operations, or transaction based operations.) 

Starvation happens when a thread needs a resource to proceed, but
can't get it. The resource is constantly tied up by other threads and
the one that needs it can't get in. The scheduling algorithm is the
problem when this happens. Look at algorithms that assure access.
* What is the atomic operation and why do they matter?
Atomic operations are operations that can't lose control of the
resources they have while executing. Functionally, they are equivalent
to operations that happen in a single clock tick on a simple
non-pipelined processor. In reality, most take more than a tick but
are protected while they execute. They are immune to race conditions,
and that makes them useful. 


* Name three thread design patterns[fn:1]
1. Thread pool
2. Peer
3. Pipeline
* Explain how a thread pool works
One thread dispatches other threads to do useful work which are
usually part of a worker thread pool. This thread pool is usually
pre-allocated before the boss(or master) begins dispatching threads to
work. Although threads are lightweight, they still incur overhead when
they are created.

If the boss thread becomes a worker thread once it's done starting
other worker threads then this is a Peer Thread Design Pattern.
* Define: critical section
The code between lock and unlock calls to a mutex.
* What are four mutex types?
+ Recursive: allows a thread holding the lock to acquire the same lock
  again which may be necessary for recursive algorithms.
+ Queuing: allows for fairness in lock acquisition by providing FIFO
  ordering to the arrival of lock requests. Such mutexes may be slower
  due to increased overhead and the possibility of having to wake
  threads next in line that may be sleeping.
+ Reader/Writer: allows for multiple readers to acquire the lock
  simultaneously. If existing readers have the lock, a writer request
  on the lock will block until all readers have given up the lock.
  This can lead to writer starvation.
+ Scoped: RAII-style semantics regarding lock acquisition and
  unlocking.

* Define: deadlock
Two (or more) threads have stopped execution or are spinning
permanently. For example, a simple deadlock situation: thread 1 locks
lock A, thread 2 locks lock B, thread 1 wants lock B and thread 2
wants lock A.
* How can you prevent deadlocking from occurring?
You can prevent deadlocks from happening by making sure threads
acquire locks in an agreed order(i.e. preservation of lock ordering).
Deadlock can also happen if threads do not unlock mutexes properly.

* Define: race condition
A race condition is when non-deterministic behavior results from
threads accessing shared data or resources without following a defined
synchronization protocol for serializing such access.
* How can you prevent race conditions from occurring?
Take steps within your program to enforce serial access to shared file
descriptors and other external resources.
* Define: priority inversion
A higher priority thread can wait behind a lower priority thread if
the lower priority thread holds a lock for which the higher priority
thread is waiting.

* Define: Condition Variable
Condition variables allow threads to synchronize to a value of a
shared resource. Typically, condition variables are used as a
notification system between threads.
* Define: (thread) barriers
Barriers are a method to synchronize a set of threads at some point in
time by having all participating threads in the barrier wait until all
threads have called the said barrier function. This, in essence,
blocks all threads participating in the barrier until the slowest
participating thread reaches the barrier call.
* Semaphores
Semaphores are another type of synchronization primitive that come in
two flavors: binary and counting. Binary semaphores act much like
simple mutexes, while counting semaphores can behave as recursive
mutexes. Counting semaphores can be initialized to any arbitrary value
which should depend on how many resources you have available for that
particular shared data. Many threads can obtain the lock
simultaneously until the limit is reached. This is referred to as lock
depth.

* Spinlocks
Spinlocks are locks which spin on mutexes. Spinning refers to
continuously polling until a condition has been met. In the case of
spinlocks, if a thread cannot obtain the mutex, it will keep polling
the lock until it is free. The advantage of a spinlock is that the
thread is kept active and does not enter a sleep-wait for mutex to
become available, thus can perform better in certain cases than
typical blocking-sleep-wait style mutexes. Mutexes which are heavily
contended are poor candidates for spinlocks.
* Six synchronization primitives
1. Mutex
2. Join
3. Condition variable
4. Barriers
5. Spinlock
6. Semaphore

* livelock
A livelock is simliar to a deadlock, except that the states of the
processes involved in the livelock constantly change with regard to
one another, none progressing.
* What does the term 'lock-free' mean?
Multithreaded code written such that the threads can never permanently
lock up.
* "Busy waiting" and how it can be avoided
When one thread is waiting for another thread using an active looping
structure that doesn't do anything. This can be avoided using mutexes.

* How would you maintain concurrency on a shared page being edited by multiple users simultaneously. 
What if the page is being shared using a client- server mechanism.
Represent the classes and explain the thread safety mechnism to avoid
editing conflicts.

Use [[http://en.wikipedia.org/wiki/Optimistic_concurrency_control][Optimistic concurrency control(OCC)]]. OCC assumes that multiple
transactions can frequently complete without interfering with each
other. While running, transactions use data resources without
acquiring locks on those resources. Before committing, each
transaction verifies that no other transaction has modified the data
it has read. If the check reveals conflicting modifications, the
committing transaction rolls back and can be restarted.

+ Divide the page into multiple segments. Assign each segment a
  version number which increments on every update.
+ The list of version numbers is communicated to the clients as well.
+ Before updating a segment, the client gets the latest page and its
  versions from the server. Updates the page and sends it across to
  the server along with the versions.
+ f any part of the page is updated, the version on the server would
  change. If so, reject the clients request along with the latest page
  and the changed versions.

* Write a multi threaded C code with one thread printing all even numbers 
and the other all odd numbers. The output should always be in sequence
ie. 0,1,2,3,4....etc



* c

http://softpixel.com/~cwright/programming/threads/threads.c.php

http://www.careercup.com/page?pid=threads-interview-questions

c++ multithreading interview questions

#+begin_src c++

#+end_src

* Footnotes

[fn:1] http://quizlet.com/24524062/c-multithreading-practice-interview-questions-flash-cards/

